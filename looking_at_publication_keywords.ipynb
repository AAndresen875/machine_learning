{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a principle component analysis looking at key words in publications and their relation to views:\n",
    "* The dependent variable we want it to predict is # of views\n",
    "* Based off the independent variable keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TB\n",
    "    \n",
    "    subgraph output\n",
    "\t\tkeywords_likely_to_produce_events\n",
    "    end\n",
    "    subgraph model_defined_with_algorithm\n",
    "\t\tprinciple_component_analysis\n",
    "    end\n",
    "    subgraph inputs\n",
    "    publication_name\n",
    "    publication_weight\n",
    "    publication_keywords\n",
    "    end\n",
    "    inputs --> model_defined_with_algorithm --> output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataset = pd.read_excel('publication_keywords_and_weight.xlsx')\n",
    "print(\"What is the python datatype of this read in excel sheet? :\", type(our_dataset))\n",
    "print(\"What is the size of this dataframe: \", our_dataset.shape)\n",
    "our_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe processing:\n",
    "Our dataframe (python's version of an excel sheet) is in a form that could use some help to make the machine be able to know what we're talking about when.\n",
    "\n",
    "Some things we're doing:\n",
    "This may not actually be needed but I'm curious so I'm doing it currently to I can also visualize the data: \n",
    "1. Extracting the 'Publication_keywords' for TF-IDF transformation\n",
    "2. Applying TF-IDF Vectorizer to convert text data into numerical format, this is something that machines like to do when working with words to get it into numbers. They find the pattern of characters (the word's spelling), and look at term frequency times the inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the publication keywords being read in as a list so it sees each word as distinct from the next keywords seperated by a comma\n",
    "# our_dataset.Publication_keywords = our_dataset.Publication_keywords.str.split(',')\n",
    "# print(our_dataset.shape)\n",
    "# our_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "Prompt I used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(dataframe):\n",
    "    # Extracting the 'Publication_keywords' for TF-IDF transformation\n",
    "    keywords = dataframe['Publication_keywords']\n",
    "    \n",
    "    # Applying TF-IDF Vectorizer to convert text data into numerical format\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(keywords)\n",
    "    \n",
    "    # Standardizing the 'publication_weight' column\n",
    "    scaler = StandardScaler()\n",
    "    publication_weight_scaled = scaler.fit_transform(dataframe[['publication_weight']])\n",
    "    \n",
    "    # Combining TF-IDF features with 'publication_weight'\n",
    "    features_combined = pd.concat([pd.DataFrame(tfidf_matrix.toarray()), pd.DataFrame(publication_weight_scaled, columns=['publication_weight'])], axis=1)\n",
    "    \n",
    "    # Performing PCA\n",
    "    pca = PCA(n_components=2)  # Adjust n_components based on the desired dimensionality reduction\n",
    "    principal_components = pca.fit_transform(features_combined)\n",
    "    \n",
    "    # Creating a DataFrame for the principal components\n",
    "    principal_df = pd.DataFrame(data=principal_components, columns=['principal_component_1', 'principal_component_2'])\n",
    "    \n",
    "    # Displaying the principal components\n",
    "    display(principal_df)\n",
    "    \n",
    "    # Identifying the features (keywords) that contribute most to the principal components\n",
    "    feature_names = vectorizer.get_feature_names_out().tolist() + ['publication_weight']\n",
    "    most_important_features = [feature_names[index] for index in pca.components_[0].argsort()[-10:][::-1]]  # Adjust the number of features as needed\n",
    "    \n",
    "    # Displaying the most important features for the first principal component\n",
    "    print(\"Most important features for the first principal component:\", most_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using model to analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming 'df' is your DataFrame)\n",
    "output = perform_pca(our_dataset)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrospective/ Things to do better next time\n",
    "1. do model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluations:\n",
    "\n",
    "Most of the time when dealing with training a data science model, you split your labeled dataset into \"test\" and \"train\" data. \n",
    "\n",
    "* the \"train\" dataset is usually bigger than the test dataset, and is used to train the data science / machine learning model. This data is labeled (in this model we have the label of points)\n",
    "* the \"test\" dataset is used to evaluate how well the model performs. The independent variable is used to predict the dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
